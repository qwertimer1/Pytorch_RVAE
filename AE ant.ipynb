{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get x values of the sine wave\n",
    "\n",
    "time        = np.arange(0, 1000, 0.1);\n",
    "\n",
    "# Amplitude of the sine wave is sine of a variable like time\n",
    "\n",
    "amplitude   = np.sin(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create multiple amplitude arrays of varying size\n",
    "amplitude_2 = amplitude * 2\n",
    "amplitude_4 = amplitude *4\n",
    "amplitude_03 = amplitude * 1/3\n",
    "amplitude_02 = amplitude * 1/2\n",
    "amplitude_8 = amplitude * 8\n",
    "amplitudes = {'amplitude_2':amplitude_2, \n",
    "              'amplitude_4':amplitude_4, \n",
    "              'amplitude_8':amplitude_8, \n",
    "              'amplitude_02':amplitude_02,\n",
    "              'amplitude_03':amplitude_03}\n",
    "\n",
    "amplitudes_nd = [amplitude_2, \n",
    "              amplitude_4, \n",
    "              amplitude_8, \n",
    "              amplitude_02,\n",
    "              amplitude_03]\n",
    "\n",
    "\n",
    "eps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(amplitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amplitude_02</th>\n",
       "      <th>amplitude_03</th>\n",
       "      <th>amplitude_2</th>\n",
       "      <th>amplitude_4</th>\n",
       "      <th>amplitude_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049917</td>\n",
       "      <td>0.033278</td>\n",
       "      <td>0.199667</td>\n",
       "      <td>0.399334</td>\n",
       "      <td>0.798667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.099335</td>\n",
       "      <td>0.066223</td>\n",
       "      <td>0.397339</td>\n",
       "      <td>0.794677</td>\n",
       "      <td>1.589355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147760</td>\n",
       "      <td>0.098507</td>\n",
       "      <td>0.591040</td>\n",
       "      <td>1.182081</td>\n",
       "      <td>2.364162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.194709</td>\n",
       "      <td>0.129806</td>\n",
       "      <td>0.778837</td>\n",
       "      <td>1.557673</td>\n",
       "      <td>3.115347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.239713</td>\n",
       "      <td>0.159809</td>\n",
       "      <td>0.958851</td>\n",
       "      <td>1.917702</td>\n",
       "      <td>3.835404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.282321</td>\n",
       "      <td>0.188214</td>\n",
       "      <td>1.129285</td>\n",
       "      <td>2.258570</td>\n",
       "      <td>4.517140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.322109</td>\n",
       "      <td>0.214739</td>\n",
       "      <td>1.288435</td>\n",
       "      <td>2.576871</td>\n",
       "      <td>5.153741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.358678</td>\n",
       "      <td>0.239119</td>\n",
       "      <td>1.434712</td>\n",
       "      <td>2.869424</td>\n",
       "      <td>5.738849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.391663</td>\n",
       "      <td>0.261109</td>\n",
       "      <td>1.566654</td>\n",
       "      <td>3.133308</td>\n",
       "      <td>6.266615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.420735</td>\n",
       "      <td>0.280490</td>\n",
       "      <td>1.682942</td>\n",
       "      <td>3.365884</td>\n",
       "      <td>6.731768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.445604</td>\n",
       "      <td>0.297069</td>\n",
       "      <td>1.782415</td>\n",
       "      <td>3.564829</td>\n",
       "      <td>7.129659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.466020</td>\n",
       "      <td>0.310680</td>\n",
       "      <td>1.864078</td>\n",
       "      <td>3.728156</td>\n",
       "      <td>7.456313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.481779</td>\n",
       "      <td>0.321186</td>\n",
       "      <td>1.927116</td>\n",
       "      <td>3.854233</td>\n",
       "      <td>7.708465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.492725</td>\n",
       "      <td>0.328483</td>\n",
       "      <td>1.970899</td>\n",
       "      <td>3.941799</td>\n",
       "      <td>7.883598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.498747</td>\n",
       "      <td>0.332498</td>\n",
       "      <td>1.994990</td>\n",
       "      <td>3.989980</td>\n",
       "      <td>7.979960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.499787</td>\n",
       "      <td>0.333191</td>\n",
       "      <td>1.999147</td>\n",
       "      <td>3.998294</td>\n",
       "      <td>7.996589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.495832</td>\n",
       "      <td>0.330555</td>\n",
       "      <td>1.983330</td>\n",
       "      <td>3.966659</td>\n",
       "      <td>7.933318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.486924</td>\n",
       "      <td>0.324616</td>\n",
       "      <td>1.947695</td>\n",
       "      <td>3.895391</td>\n",
       "      <td>7.790781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.473150</td>\n",
       "      <td>0.315433</td>\n",
       "      <td>1.892600</td>\n",
       "      <td>3.785200</td>\n",
       "      <td>7.570401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.454649</td>\n",
       "      <td>0.303099</td>\n",
       "      <td>1.818595</td>\n",
       "      <td>3.637190</td>\n",
       "      <td>7.274379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.431605</td>\n",
       "      <td>0.287736</td>\n",
       "      <td>1.726419</td>\n",
       "      <td>3.452837</td>\n",
       "      <td>6.905675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.404248</td>\n",
       "      <td>0.269499</td>\n",
       "      <td>1.616993</td>\n",
       "      <td>3.233986</td>\n",
       "      <td>6.467971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.372853</td>\n",
       "      <td>0.248568</td>\n",
       "      <td>1.491410</td>\n",
       "      <td>2.982821</td>\n",
       "      <td>5.965642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.337732</td>\n",
       "      <td>0.225154</td>\n",
       "      <td>1.350926</td>\n",
       "      <td>2.701853</td>\n",
       "      <td>5.403705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.299236</td>\n",
       "      <td>0.199491</td>\n",
       "      <td>1.196944</td>\n",
       "      <td>2.393889</td>\n",
       "      <td>4.787777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.257751</td>\n",
       "      <td>0.171834</td>\n",
       "      <td>1.031003</td>\n",
       "      <td>2.062005</td>\n",
       "      <td>4.124011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.213690</td>\n",
       "      <td>0.142460</td>\n",
       "      <td>0.854760</td>\n",
       "      <td>1.709520</td>\n",
       "      <td>3.419039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.167494</td>\n",
       "      <td>0.111663</td>\n",
       "      <td>0.669976</td>\n",
       "      <td>1.339953</td>\n",
       "      <td>2.679905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.119625</td>\n",
       "      <td>0.079750</td>\n",
       "      <td>0.478499</td>\n",
       "      <td>0.956997</td>\n",
       "      <td>1.913995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>-0.448984</td>\n",
       "      <td>-0.299322</td>\n",
       "      <td>-1.795935</td>\n",
       "      <td>-3.591870</td>\n",
       "      <td>-7.183740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>-0.468707</td>\n",
       "      <td>-0.312471</td>\n",
       "      <td>-1.874829</td>\n",
       "      <td>-3.749657</td>\n",
       "      <td>-7.499314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>-0.483747</td>\n",
       "      <td>-0.322498</td>\n",
       "      <td>-1.934989</td>\n",
       "      <td>-3.869979</td>\n",
       "      <td>-7.739958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>-0.493954</td>\n",
       "      <td>-0.329303</td>\n",
       "      <td>-1.975817</td>\n",
       "      <td>-3.951633</td>\n",
       "      <td>-7.903266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>-0.499225</td>\n",
       "      <td>-0.332817</td>\n",
       "      <td>-1.996902</td>\n",
       "      <td>-3.993804</td>\n",
       "      <td>-7.987608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>-0.499509</td>\n",
       "      <td>-0.333006</td>\n",
       "      <td>-1.998035</td>\n",
       "      <td>-3.996070</td>\n",
       "      <td>-7.992140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>-0.494801</td>\n",
       "      <td>-0.329867</td>\n",
       "      <td>-1.979204</td>\n",
       "      <td>-3.958409</td>\n",
       "      <td>-7.916817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>-0.485150</td>\n",
       "      <td>-0.323433</td>\n",
       "      <td>-1.940598</td>\n",
       "      <td>-3.881196</td>\n",
       "      <td>-7.762392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>-0.470650</td>\n",
       "      <td>-0.313767</td>\n",
       "      <td>-1.882602</td>\n",
       "      <td>-3.765204</td>\n",
       "      <td>-7.530408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.300966</td>\n",
       "      <td>-1.805796</td>\n",
       "      <td>-3.611591</td>\n",
       "      <td>-7.223182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>-0.427737</td>\n",
       "      <td>-0.285158</td>\n",
       "      <td>-1.710946</td>\n",
       "      <td>-3.421893</td>\n",
       "      <td>-6.843785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>-0.399750</td>\n",
       "      <td>-0.266500</td>\n",
       "      <td>-1.599002</td>\n",
       "      <td>-3.198004</td>\n",
       "      <td>-6.396007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>-0.367770</td>\n",
       "      <td>-0.245180</td>\n",
       "      <td>-1.471081</td>\n",
       "      <td>-2.942161</td>\n",
       "      <td>-5.884322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>-0.332115</td>\n",
       "      <td>-0.221410</td>\n",
       "      <td>-1.328461</td>\n",
       "      <td>-2.656922</td>\n",
       "      <td>-5.313843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>-0.293142</td>\n",
       "      <td>-0.195428</td>\n",
       "      <td>-1.172568</td>\n",
       "      <td>-2.345135</td>\n",
       "      <td>-4.690270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>-0.251240</td>\n",
       "      <td>-0.167493</td>\n",
       "      <td>-1.004958</td>\n",
       "      <td>-2.009917</td>\n",
       "      <td>-4.019833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>-0.206827</td>\n",
       "      <td>-0.137885</td>\n",
       "      <td>-0.827308</td>\n",
       "      <td>-1.654616</td>\n",
       "      <td>-3.309232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>-0.160348</td>\n",
       "      <td>-0.106899</td>\n",
       "      <td>-0.641391</td>\n",
       "      <td>-1.282783</td>\n",
       "      <td>-2.565565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>-0.112267</td>\n",
       "      <td>-0.074844</td>\n",
       "      <td>-0.449066</td>\n",
       "      <td>-0.898132</td>\n",
       "      <td>-1.796265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>-0.063064</td>\n",
       "      <td>-0.042042</td>\n",
       "      <td>-0.252254</td>\n",
       "      <td>-0.504508</td>\n",
       "      <td>-1.009016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>-0.013230</td>\n",
       "      <td>-0.008820</td>\n",
       "      <td>-0.052922</td>\n",
       "      <td>-0.105843</td>\n",
       "      <td>-0.211686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>0.036735</td>\n",
       "      <td>0.024490</td>\n",
       "      <td>0.146940</td>\n",
       "      <td>0.293880</td>\n",
       "      <td>0.587759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>0.086333</td>\n",
       "      <td>0.057555</td>\n",
       "      <td>0.345333</td>\n",
       "      <td>0.690666</td>\n",
       "      <td>1.381332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>0.135069</td>\n",
       "      <td>0.090046</td>\n",
       "      <td>0.540276</td>\n",
       "      <td>1.080551</td>\n",
       "      <td>2.161102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>0.182455</td>\n",
       "      <td>0.121637</td>\n",
       "      <td>0.729820</td>\n",
       "      <td>1.459640</td>\n",
       "      <td>2.919280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.228018</td>\n",
       "      <td>0.152012</td>\n",
       "      <td>0.912072</td>\n",
       "      <td>1.824145</td>\n",
       "      <td>3.648289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.271303</td>\n",
       "      <td>0.180869</td>\n",
       "      <td>1.085212</td>\n",
       "      <td>2.170423</td>\n",
       "      <td>4.340846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.311877</td>\n",
       "      <td>0.207918</td>\n",
       "      <td>1.247508</td>\n",
       "      <td>2.495015</td>\n",
       "      <td>4.990031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.349335</td>\n",
       "      <td>0.232890</td>\n",
       "      <td>1.397339</td>\n",
       "      <td>2.794678</td>\n",
       "      <td>5.589356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.383302</td>\n",
       "      <td>0.255535</td>\n",
       "      <td>1.533209</td>\n",
       "      <td>3.066417</td>\n",
       "      <td>6.132835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      amplitude_02  amplitude_03  amplitude_2  amplitude_4  amplitude_8\n",
       "0         0.000000      0.000000     0.000000     0.000000     0.000000\n",
       "1         0.049917      0.033278     0.199667     0.399334     0.798667\n",
       "2         0.099335      0.066223     0.397339     0.794677     1.589355\n",
       "3         0.147760      0.098507     0.591040     1.182081     2.364162\n",
       "4         0.194709      0.129806     0.778837     1.557673     3.115347\n",
       "5         0.239713      0.159809     0.958851     1.917702     3.835404\n",
       "6         0.282321      0.188214     1.129285     2.258570     4.517140\n",
       "7         0.322109      0.214739     1.288435     2.576871     5.153741\n",
       "8         0.358678      0.239119     1.434712     2.869424     5.738849\n",
       "9         0.391663      0.261109     1.566654     3.133308     6.266615\n",
       "10        0.420735      0.280490     1.682942     3.365884     6.731768\n",
       "11        0.445604      0.297069     1.782415     3.564829     7.129659\n",
       "12        0.466020      0.310680     1.864078     3.728156     7.456313\n",
       "13        0.481779      0.321186     1.927116     3.854233     7.708465\n",
       "14        0.492725      0.328483     1.970899     3.941799     7.883598\n",
       "15        0.498747      0.332498     1.994990     3.989980     7.979960\n",
       "16        0.499787      0.333191     1.999147     3.998294     7.996589\n",
       "17        0.495832      0.330555     1.983330     3.966659     7.933318\n",
       "18        0.486924      0.324616     1.947695     3.895391     7.790781\n",
       "19        0.473150      0.315433     1.892600     3.785200     7.570401\n",
       "20        0.454649      0.303099     1.818595     3.637190     7.274379\n",
       "21        0.431605      0.287736     1.726419     3.452837     6.905675\n",
       "22        0.404248      0.269499     1.616993     3.233986     6.467971\n",
       "23        0.372853      0.248568     1.491410     2.982821     5.965642\n",
       "24        0.337732      0.225154     1.350926     2.701853     5.403705\n",
       "25        0.299236      0.199491     1.196944     2.393889     4.787777\n",
       "26        0.257751      0.171834     1.031003     2.062005     4.124011\n",
       "27        0.213690      0.142460     0.854760     1.709520     3.419039\n",
       "28        0.167494      0.111663     0.669976     1.339953     2.679905\n",
       "29        0.119625      0.079750     0.478499     0.956997     1.913995\n",
       "...            ...           ...          ...          ...          ...\n",
       "9970     -0.448984     -0.299322    -1.795935    -3.591870    -7.183740\n",
       "9971     -0.468707     -0.312471    -1.874829    -3.749657    -7.499314\n",
       "9972     -0.483747     -0.322498    -1.934989    -3.869979    -7.739958\n",
       "9973     -0.493954     -0.329303    -1.975817    -3.951633    -7.903266\n",
       "9974     -0.499225     -0.332817    -1.996902    -3.993804    -7.987608\n",
       "9975     -0.499509     -0.333006    -1.998035    -3.996070    -7.992140\n",
       "9976     -0.494801     -0.329867    -1.979204    -3.958409    -7.916817\n",
       "9977     -0.485150     -0.323433    -1.940598    -3.881196    -7.762392\n",
       "9978     -0.470650     -0.313767    -1.882602    -3.765204    -7.530408\n",
       "9979     -0.451449     -0.300966    -1.805796    -3.611591    -7.223182\n",
       "9980     -0.427737     -0.285158    -1.710946    -3.421893    -6.843785\n",
       "9981     -0.399750     -0.266500    -1.599002    -3.198004    -6.396007\n",
       "9982     -0.367770     -0.245180    -1.471081    -2.942161    -5.884322\n",
       "9983     -0.332115     -0.221410    -1.328461    -2.656922    -5.313843\n",
       "9984     -0.293142     -0.195428    -1.172568    -2.345135    -4.690270\n",
       "9985     -0.251240     -0.167493    -1.004958    -2.009917    -4.019833\n",
       "9986     -0.206827     -0.137885    -0.827308    -1.654616    -3.309232\n",
       "9987     -0.160348     -0.106899    -0.641391    -1.282783    -2.565565\n",
       "9988     -0.112267     -0.074844    -0.449066    -0.898132    -1.796265\n",
       "9989     -0.063064     -0.042042    -0.252254    -0.504508    -1.009016\n",
       "9990     -0.013230     -0.008820    -0.052922    -0.105843    -0.211686\n",
       "9991      0.036735      0.024490     0.146940     0.293880     0.587759\n",
       "9992      0.086333      0.057555     0.345333     0.690666     1.381332\n",
       "9993      0.135069      0.090046     0.540276     1.080551     2.161102\n",
       "9994      0.182455      0.121637     0.729820     1.459640     2.919280\n",
       "9995      0.228018      0.152012     0.912072     1.824145     3.648289\n",
       "9996      0.271303      0.180869     1.085212     2.170423     4.340846\n",
       "9997      0.311877      0.207918     1.247508     2.495015     4.990031\n",
       "9998      0.349335      0.232890     1.397339     2.794678     5.589356\n",
       "9999      0.383302      0.255535     1.533209     3.066417     6.132835\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from torchaudio import transforms\n",
    "#from torchaudio import Datasets\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from glob import glob\n",
    "import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import comet_ml in the top of your file\n",
    "from comet_ml import Experiment\n",
    "##Needs to be imported before sklearn\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import scipy\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nd_array = np.ndarray(amplitudes_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch_tensor = torch.tensor(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.type>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch_tensor = torch_tensor.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Instance of Dataset with additional code to create npy file with MFCCs should fix later.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Need to create a config inoput for this. It is unwieldy\n",
    "    def __init__(self, data \n",
    "                ):\n",
    "        self.data = data\n",
    "        pass\n",
    "\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        #Reads in dimension 1 of the 3-D x_data array(width of dataset, length of MFCC, MFCC amount)\n",
    "       \n",
    "        return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper_Params\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-6\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = TensorsDataset(torch_tensor.float())\n",
    "\n",
    "train_loader= torch.utils.data.DataLoader(train, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0499,  0.0993,  ...,  0.3119,  0.3493,  0.3833]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vals = dataiter.next()\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.rnn1 = nn.LSTM(10000, 400, num_layers = 2)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(20, 20)\n",
    "        self.rnn3 = nn.LSTM(20, 400, num_layers = 1)\n",
    "        self.rnn4 = nn.LSTM(400, 10000, num_layers = 2)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.rnn1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.rnn3(z))\n",
    "        return F.sigmoid(self.rnn4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 1000\n",
    "input_size = 1\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "\n",
    "        \n",
    "        print(data.shape)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.view(-1,1,1)\n",
    "            \n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-e6eab45e804f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m#test(epoch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-134-14406c70db0f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-132-2f190c894db0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-132-2f190c894db0>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc21\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc22\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[0mflat_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         func = self._backend.RNN(\n\u001b[0;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    124\u001b[0m             raise RuntimeError(\n\u001b[0;32m    125\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[1;32m--> 126\u001b[1;33m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             raise RuntimeError(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "for epoch in range(1000):\n",
    "    train(epoch)\n",
    "    #test(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
